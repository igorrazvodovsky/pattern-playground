import { Meta } from '@storybook/addon-docs/blocks';

<Meta title="Patterns/Bot" />

> ðŸ™‚ **Fun meter: 4/5**. A part of a bigger attempt to map LLM-related patterns.
For now I stick with â€˜botâ€™ over â€˜agentâ€™ or â€˜assistantâ€™ because those tend to carry baggage about capabilities and autonomy.

# Bot

A bot interprets actor intent and returns relevant outputâ€”text, UI, or actions. It acts as an adaptable mediator between actor goals and system functionality, especially when intent is fuzzy or workflows are complex. This umbrella term encompasses all user-facing AI interactionsâ€”from chatbots to agentic systems and beyond.

## Agency

Bot patterns embody different levels of system agency from the agency [spectrum](../?path=/docs/qualities-agency--docs#spectrum):

1. Chatbot: User-driven interaction requiring explicit prompts
2. Inline: Assistance mode providing contextual help and smart suggestions
3. Workflow automation: autonomous action that follows predefined sequence
4. Full autonomy: proactive monitoring and acting without direct user input

Higher agency modes require more careful preservation of [user agency](../?path=/docs/qualities-agency--docs) to maintain control and transparency.

## Modes of interaction

- Focused: Bot as a primary interface, typically via [messaging](../?path=/docs/compositions-messaging--docs).
- Inline: Contextual interactions embedded directly in the UI, such as autocompletions, rewrite suggestions, or smart defaults.
- Ambient: Behind-the-scenes monitoring or support, offering proactive help or taking actions without direct prompting.

## Variants

### Single-turn interactions

Simple query-response patterns without state persistence or tool use. Classic chatbots often operate in this mode.

### Multi-turn conversations

Maintaining context across exchanges, allowing for clarification, follow-ups, and evolving understanding.

### Tool-augmented agents

Bots that can invoke external capabilitiesâ€”from simple calculations to complex API calls. This includes the *bounded loop execution* pattern where agents run tools iteratively until reaching objectives.

{/* ### Autonomous workflows */}
{/* Self-directed execution following predefined or learned patterns, with varying degrees of human oversight. */}

## Memory

### Short-term

Conversation context provides implicit short-term memory, allowing bots to maintain state across interactions within a session.

### Long-term

Persistent memory requires explicit implementation through:
- Storage tools (databases, files, vector stores)
- Retrieval mechanisms with semantic search
- Memory management strategies to prevent unbounded growth

{/* ## Functional flexibility */}

{/* Bots adapt their behaviour across a spectrum of capabilities: */}

{/* ### Response generation */}

{/* - Template-based: Structured responses from predefined patterns */}
{/* - Retrieval-augmented: Combining learned knowledge with external information */}
{/* - Generative: Creating novel responses from learned distributions */}
{/* - Hybrid approaches: Mixing deterministic and probabilistic elements */}

{/* ### Task orchestration */}

{/* - Linear execution: Step-by-step progression through defined workflows */}
{/* - Branching logic: Conditional paths based on context and outputs */}
{/* - Recursive decomposition: Breaking complex tasks into manageable subtasks */}
{/* - Emergent planning: Dynamic strategy formation based on intermediate results */}

{/* ### Confidence mapping */}

{/* Integrated with *input-output mapping*, it's aimed to increase transparency by indicating the bot/model's confidence in different parts of its response. */}

## Anti-patterns

### Algorithmic loafing

When bots take creative or cognitive lead, humans reduce effort and disengageâ€”accepting bot output with minimal reflection. This manifests as:

- Reduced cognitive engagement despite task importance
- Loss of skill development and expertise over time
- Decreased perceived ownership and commitment to outcomes
- Homogenisation of outputs across users

The effect parallels [social loafing](https://en.wikipedia.org/wiki/Social_loafing), where individuals reduce contribution when responsibility diffuses.

*Mitigation*: Design interactions where humans remain in creative or cognitive lead whilst bot provides scaffolding through questions, analogies, or contextual knowledge rather than complete solutions. See [Conversation](../?path=/docs/patterns-conversation--docs) dialogue forms for question-driven approaches that maintain engagement.

### Mismatched scope

Applying bots to tasks requiring genuine accountability, sustained comprehension, or human empathy. Bots cannot take responsibility for actions or form independent intentions, and their understanding operates within learned patterns without true comprehension.

### Uncalibrated trust

Treating bot outputs as authoritative without verification mechanisms. Bots can generate plausible but incorrect information, and performance degrades with extended interactions or complex state.

### Undefined boundaries

Failing to communicate bot capabilities and limitations, leading to misuse and frustration. Users need clear understanding of what bots can and cannot do to set appropriate expectations.

## Related patterns

### Precursors
{/* Primarily establish clear expectations, prepare mental models, and facilitate effective input to enhance subsequent bot interactions. */}

- [Agency](../?path=/docs/qualities-agency--docs) - how much freedom the bot has to act on its own.
- [Onboarding](../?path=/docs/patterns-onboarding--docs), priming, and [suggestions](../?path=/docs/patterns-suggestion--docs) are the first steps that help people get to know the botâ€”what it can and can't do, and how to get the most out of it.
- [Progressive disclosure](../?path=/docs/patterns-progressive-disclosure--docs) â€“ bots often operate well with incremental complexity. Progressive disclosure keeps things simple at first and helps actors build confidence.

### Follow-ups

- Bot can use [messaging](../?path=/docs/compositions-messaging--docs) to [converse](../?path=/docs/patterns-conversation--docs) with actors.
- [Prompt](../?path=/docs/patterns-prompt--docs) describes ways of providing input to the bot.
- [Formality](../?path=/docs/qualities-formality--docs) â€“ bots accept unstructured input by design; structure can be inferred or gradually formalised from interaction.
- LLM [settings](../?path=/docs/patterns-intelligence-llm-settings--docs) help keep users in control while the bot does its thing.
- [Generated content](../?path=/docs/patterns-generated-content--docs) - presentation and manipulation of AI's output.
- [Transparent reasoning](../?path=/docs/patterns-transparent-reasoning--docs) makes bot's decision-making process visible and inspectable.
- [Activity log](../?path=/docs/patterns-activity-log--docs) tracks bot actions and reasoning for transparency and review.
- [Collaboration](../?path=/docs/patterns-collaboration--docs) - Bots participate as collaborative agents with adaptable mediation capabilities

{/* ### Concepts */}
{/* - [Task](../?path=/docs/concepts-task--docs) - Bots interpret task specifications and execute work using available tools and capabilities */}

{/* Input assistance - predictive text, autocomplete, and smart suggestions that aid actor input. */}
{/* TODO: alt. for inline and ambient modes */}


## Resources & references

- Maier, Schneider, Feuerriegel (2025) [Partnering with Generative AI: Experimental evaluation of human-led and model-led interaction in human-AI co-creation](https://arxiv.org/abs/2510.23324)
- Nardi, B., Miller, J. R., & Wright, D. J. (1998) [Collaborative, programmable intelligent agents](https://dl.acm.org/doi/10.1145/272287.272331). Communications of the ACM, 41(3), 96â€“104.
- Thu, Kocaballi (2025) [Personalization features for Human-LLM Interactions](https://www.arxiv.org/pdf/2503.00681)
- Simon Willison (2025) [Agents](https://simonwillison.net/2025/Sep/18/agents/)
- Amelia Wattenberger (2024) [Why chatbots are not the future](https://wattenberger.com/thoughts/boo-chatbots)
- Luke Wroblewski (2023) [Expanding conversational user interfaces](https://www.lukew.com/ff/entry.asp?2018)
- Andy Matuschak (2025) [How might we learn?](https://andymatuschak.org/hmwl/) â€“ guidance in action and the critique of transactional chatbot tutoring