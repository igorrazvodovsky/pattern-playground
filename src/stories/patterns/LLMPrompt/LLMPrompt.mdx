import { Meta, Canvas } from '@storybook/addon-docs/blocks';
import * as LLMPromptStories from './LLMPrompt.stories.tsx';

<Meta of = { LLMPromptStories } />

> ðŸ™‚ **Fun meter: 5/5**. A part of a bigger attempt to map LLM-related patterns.

# Prompt

An input for a [bot](../?path=/docs/patterns-bot--docs).

## Intent

explicit and implicit...

## Input mechanisms
<div>
- (unstructured) natural language
- structured prompt
- voice
- image
- reference materials: both @mentions and attachments
</div>

{/* Redo using blocks */}
<Canvas of={ LLMPromptStories.StructuredPrompt } />

## Reference materials with @mentions

Using @mentions to reference documents, images, code, and other materials provides context for better LLM responses.

<Canvas of={ LLMPromptStories.WithMaterialReferences } />

### Empty prompt with mention capabilities

<Canvas of={ LLMPromptStories.EmptyPromptWithMentions } />

## Prompt guidance and scaffolding

Bridging the gap between fuzzy idea inside a human's head and what makes an effective prompt for LLMs.

### Quality feedback

<Canvas of={ LLMPromptStories.QualityFeedback } />

### Editing assistance

### Templates

...

### Suggestions

...

### Prompt evaluation

...

## Related patterns

### Precursor patterns

<div>
- [Bot](../?path=/docs/patterns-bot--docs) â€“ a thing that gets prompted.
- [Suggestion](../?path=/docs/patterns-suggestion--docs) can provide an initial prompt.
- session, thread, etc.
</div>

### Follow-up patterns

<div>
- [Settings](../?path=/docs/patterns-llm-settings--docs) let users refine the way LLM gets prompted.
- [Result](../?path=/docs/patterns-llm-result--docs) â€“ what (and how) user gets from the model.
</div>

{/* ### Complementary patterns */}

{/* - LLM model selection */}
{/* - [Messaging](../?path=/docs/compositions-messaging--docs) */}
{/* - [Suggestion](../?path=/docs/patterns-suggestion--docs) */}

## Resources & references

<div>
- [shapeof.ai](https://www.shapeof.ai/)
- [aiuxpatterns.com](https://aiuxpatterns.com/)
</div>