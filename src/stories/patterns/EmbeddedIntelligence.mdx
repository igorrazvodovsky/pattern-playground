import { Meta, Story, Canvas } from '@storybook/addon-docs/blocks';

<Meta title='Patterns/Embedded intelligence' />

# Embedded intelligence

Embedded intelligence integrates LLMs directly into familiar workflows and interfaces, not as standalone tools but as extensions of users’ existing ways of thinking and acting.

## Understanding intent

Translating user's fuzzy, high-level goal into a precise, structured command the system can execute.
This process involves two key steps:

### Intent Detection

The system identifies signals that indicate a user's goal. These signals can be:

- Keywords and natural language: Direct text input.
- User actions: Selecting text, dragging an element, hovering over an item for a certain duration.
- Navigation paths: The sequence of pages or views a user visits can imply a larger goal.
- Contextual cues: The current state of the application, the type of data being viewed, or the user's role.
- User history

### Intent interpretation
{/* Once a signal is detected, the system interprets it to infer the desired outcome. Basically pairing it with system prompt */}
...

## Examples

{/* ### LLM-Powered Filtering */}
{/* - **Detection:** The user types a natural language query like "high priority tasks for next week" into a filter input. */}
{/* - **Interpretation:** The LLM parses this query, understands the concepts of "high priority" and "next week," and translates them into structured filter criteria (e.g., `Priority = High` AND `DueDate < now() + 7d`). */}
{/* - **Result:** The system applies the corresponding filter, fulfilling the user's intent without forcing them to manually construct the query. */}

## Related patterns

### Precursors

- Extends [adaptation](../?path=/docs/foundations-adaptation--docs) by mixing deterministic (rule-driven) and probabilistic (inferred or predicted) responses, creating dynamic, context-sensitive interactions.
- Enhances the system’s [agency](../?path=/docs/foundations-agency--docs) by enabling autonomous decisions and actions.

### Follow-ups & Complements

- **[Command menu](../?path=/docs/patterns-command-menu--docs):** A natural home for embedded intelligence, turning a simple command line into an intent-driven interface.
- **[Filtering](../?path=/docs/compositions-filtering--docs):** A prime example of embedded intelligence, where natural language queries are translated into structured filters.
- **[Focus and context](../?path=/docs/patterns-focus-and-context--docs):** Can use embedded intelligence to infer what contextual information is most relevant to a user's current focus.
- **[Suggestion](../?path=/docs/patterns-suggestion--docs):** The output of embedded intelligence is often a suggestion that the user can accept, reject, or modify.
- **Continuous feedback loop:** The user's reaction to the embedded intelligence (e.g., accepting or rejecting a suggestion) provides valuable feedback for improving the underlying model.

## Resources & references

- Heer, J. (2019). [Agency plus automation: Designing artificial intelligence into interactive systems](https://www.pnas.org/doi/10.1073/pnas.1807184115). *PNAS*.
- Salovaara, A. (2004). Six modes of proactive resource management.
- Noessel, C. (2017). Designing agentive technology.
- Tennenhouse, D. (2000). Proactive computing.