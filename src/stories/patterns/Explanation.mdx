import { Meta, Story } from '@storybook/addon-docs/blocks';
import { DynamicExplanation } from '../components/BubbleMenu.stories';

<Meta title="Patterns/Explanation" />

> ðŸ’­ **Fun meter:** _calculating..._

# Explanation

Provides actors with contextual information to clarify system behaviours, human actions, and AI-driven decisions to foster transparency and user trust.

## Types of explanation

### Explaining (deterministic) system logic
Rules, workflows, why certain options appear, validation errors, etc.

#### Examples

- Explaining why an action is unavailable
- Clarifying validation errors or required fields in forms
- Communicating the next steps or expected actor actions in workflows

### Domain explanations
Connecting system behaviour to real-world domain concepts.
In complex systems, the interface embeds domain knowledgeâ€”process logic, regulatory requirements, industry practices.
Domain explanations surface this embedded knowledge.

#### Examples

- Explaining why a workflow exists (regulatory requirement, industry standard, organisational policy)
- Clarifying domain terminology embedded in field labels or options
- Connecting data structures to the real-world entities they represent
- Grounding validation rules in domain constraints, not arbitrary system rules

Domain explanations transform system constraints from arbitrary-feeling to meaningful. They support [domain learnability](../?path=/docs/foundations-learnability--docs)â€”helping users understand the domain through the tool.

### Social/Collaboration explanations
Actions taken by other actors, notifications about shared resources, edits, status changes, etc.
Transparency regarding the actions of other people to reduce uncertainty in collaborative or multi-actor environments.

#### Examples

- Explaining edits or changes made by another actor
- Providing context about shared resources or history
- Clarifying reasons for notifications or status updates initiated by other people

### AI explanations
How the system arrived at a prediction, recommendation, or generated output.

#### Examples

- Explaining recommendation or prediction logic ("Why am I seeing this?")
- Clarifying the limitations or confidence levels of AI-generated outputs
- Communicating factors influencing automated decisions or outputs

### Dynamic explanation

On-demand explanations for any selected content, supporting contextualised learning.

<Story of={DynamicExplanation} />

When users select content, explanations can address different aspects:
- Interface mechanics: *how* to use this element
- Domain significance: *why* it exists and what it represents
- Personal relevance: connecting to the user's actual data and context
- World knowledge: bridging system concepts to broader domain understanding

{/* Reference: https://andymatuschak.org/hmwl/#contextualized-study */}


## Amount of detail

1. Bare minimum: Brief, direct explanation for common scenarios
2. Moderate detail: expanded explanations providing additional context, suitable for actors needing further clarity or reassurance.
3. Extended report: explanations offering full transparency, suitable for complex decisions, troubleshooting, or expert actors who require in-depth understanding.

[Progressive disclosure](../?path=/docs/patterns-progressive-disclosure--docs)
can be used to assist actors in transitioning between different levels of detail.

## Components

### Level 1. Indicator

#### Global

Describes a set of elements, aiming to convey the broader context influencing multiple objects.

TODO: Example: item ranking

#### Local

Explains a single value or item.

TODO: Example: Predicted match of an item
{/* Confidence Indicator */}

### Level 2. Simple explanation

Users can access the explanation popover by clicking the explanation indicator. This popover offers contextual information beyond a single value and lists the factors that contributed to the result.

TODO: Example: explain price through costs, margin, and discount

#### Natural language Explanations

TODO: Examples: "I'm not sure. But...", "... Plese keep in mind, this information is uncertain.",

### Level 3. Extended explanation

TODO: Example: drawer with instructions or help text.

## Related patterns

- [Learnability](../?path=/docs/foundations-learnability--docs) â€“ domain explanations support domain learnability
- [Disabled state](./?path=/docs/patterns-states-disabled-state--docs)
- [Progressive disclosure](../?path=/docs/patterns-progressive-disclosure--docs)
- [Transparent reasoning](../?path=/docs/patterns-transparent-reasoning--docs) â€“ shows step-by-step process while explanation encourages understanding through interaction
- [Popover](./?path=/docs/primitives-popover--docs)
- [Drawer](./?path=/docs/components-drawer--docs)
- [Collaboration](../?path=/docs/patterns-collaboration--docs) â€“ uses explanations to facilitate understanding through transparency about other actors' actions

## Resources

- Andy Matuschak (2025) [How might we learn?](https://andymatuschak.org/hmwl/) â€“ contextualised study and dynamic explanations grounded in user's actual work
- Wikipedia / [Explainable artificial intelligence](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence)
- Mayer, R. E., & Moreno, R. (2003). [Nine ways to reduce cognitive load in multimedia learning](https://www.tandfonline.com/doi/abs/10.1207/S15326985EP3801_6). Educational Psychologist, 38(1), 43-52.
