import { Meta } from '@storybook/addon-docs/blocks';

<Meta title="Foundations/Agency" />

> ðŸ¤¯ **Fun meter: 5/5**. Trying to find useful framing to integrate LLMs with design systems.

# Agency

The distribution of decision-making and control between system and user within an interface.

## Actors

- *System agency* is the ability of a system to act independently, make decisions, and execute tasks on behalf of the user.
- *User agency* is the user's ability to maintain control, override system decisions, understand system behaviour, and retain their expertise and judgement.

## Components

 - Autonomy,
 - [Adaptation](../?path=/docs/foundations-adaptation--docs)
 - Interactivity

## Distribution

Agency distribution describes how decision-making authority and control are allocated between actors within a system. Three dimensions shape this distribution:

### Locus

Where primary decision-making authority resides:
- Human-centric: human retains ultimate authority, system serves in assistive role
- Shared: authority distributed between human and system as collaborators
- System-centric: system executes autonomously within human-defined parameters

### Dynamics

How authority is managed over time:
- Static allocation: predefined roles and control levels set during design
- Dynamic allocation: control shifts situationally based on context and evolving factors

### Granularity

Level of detail at which authority is exercised:
- High-level: strategic goals and overall workflow direction
- Fine-grained: specific actions and parameter adjustments within processes

## Spectrum

System agency operates on a five-level spectrum, with each level building upon the previous while adding new capabilities:

Passive â†’ Reactive â†’ Semi-active â†’ Proactive â†’ Co-operative

- Passive: System acts only when explicitly invoked by user.
- Reactive: User-initiated actions trigger system responses, providing real-time assistance without independent initiative.
- Semi-active: System provides contextual support when specific conditions are met.
- Proactive: System initiates actions and introduces insights based on analysis.
- Co-operative: System collaborates as equal partner in problem-solving.

Agency patterns may shift across different contexts and stages. For example, in collaborative workflows, a system might operate reactively during initial input gathering, shift to co-operative during refinement, and become semi-active during final evaluation. See [Collaboration](../?path=/docs/patterns-collaboration--docs#human--bot) for how these agency patterns shift across interaction stages in human-AI co-creation.

As system agency increases, preserving user agency becomes more critical to maintain user control and prevent skill atrophy. Each level requires increasing trust between user and system, with co-operative modes demanding the highest levels of mutual understanding and shared mental models.

## Control patterns

Various patterns enable users to exert control and preserve agency as system autonomy increases:

### Input: information access
#### Guided Input Interaction
Patterns that structure and facilitate how users provide input to AI.
- User-centered input optimization enhances users control to align AI outputs with expectations. Users actively refine prompts iteratively based on AI responses or tailor their experience via customizable interface elements like adjustable parameters and views
- Interface-supported input guidance employs interface design to actively direct user input, improving interaction
efficiency. Interfaces provide interactive visual feedback (e.g., confidence highlighting, code comparison) to help users understand and refine inputs.

#### Context awareness and memory
Patterns that enable AI to leverage historical, environmental, task, and user information, enhancing contextual understanding and collaboration
continuity, encompassing several detailed approaches.
- interaction history integration â€“ incorporate past conversational or operational history into decision-making
- task-oriented context management â€“ maintain awareness of specific tasks, objectives, and histories to ensure consistency. System might enable retrieving artifacts for context-aware iteration, track design goals, adapt interfaces to workflow stages, or use
history to inform alerts.
- personalized context adaptation â€“ AI tailors its behavior to individual user backgrounds, needs and preferences.

#### Transparency and Explainability
Enhance user understanding of AI operations and build trust.
- interaction tracking, systems document and present interaction histories, allowing users to review contributions from both humans and AI.
- decision visualization â€“ visualize AIâ€™s decision-making processes or influencing factors. For instance, provide users with clear interface suggestions, or make algorithmic processes transparent by visually linking decision/actions to results.
- system explanation â€“ explain the reasoning behind AI-generated outputs to clarify internal workings. Examples include providing rationales for generated content, documenting iterative project changes to track solution evolution, surfacing relevant reports to explain potential
harms.

### Action exploration & coordination

#### Multimodal action space exploration
Patterns facilitateting human-AI collaboration by enabling interaction through diverse modalities.
- Text-based interaction
- Combined and integrated modalities explicitly merge interaction modes, e.g. integrating goal-setting, messaging,
and custom representations.

#### Action Coordination
How human-AI co-creation systems distribute responsibilities, decision-making authority, and interaction dynamics based on the assumed roles of human and AI agents.
- complementary role distribution â€“ systems assign distinct, interdependent roles leveraging respective strength. Humans typically provide strategic direction or creative input, while AI handles data processing or routine tasks
- human-dominated agency with AI support â€“ humans retain primary decision-making authority, utilizing AI as an auxiliary tool or assistant
that provides suggestions or analysis but lacks autonomous agency
- shared creative agency â€“ humans and AI engage jointly with mutual influence on the creative output. Both participate throughout
idea generation, refinement and evaluation, fostering an emergent process
- technical precision and control â€“ systems focus on enabling users to exert detailed, fine-grained control over AI outputs
through interfaces offering parameter tuning, prompt refinement, or direct manipulation
- autonomous AI contribution â€“ AI operates with significant autonomy on specific sub-tasks within parameters established by humans, contributing independently while remaining accountable to human oversight.

### Output: direct intervention

#### Modification and Intervention
Detail how users exert control over AI systems by intervening in processes or modifying outputs.
- direct editing and adjustment â€“ user directly alter AI outputs. This includes manually labeling or modifying generated content, or identifying and correcting errors.
- parameter and prompt control â€“ users influence AI outcomes indirectly through system settings or inputs, rather than altering the output itself. Examples include shaping generative processes via prompts and parameters, controlling the sequence or level of AI assistance [45], or adjusting AI decision-making parameters.
- real-time intervention and adjustment â€“ users intervene during an ongoing AI process.
- acceptance or rejection or AI suggestions â€“ users act as gatekeepers by explicitly accepting or rejecting AI
contributions. This includes users overriding suggestions and documenting their own assessments, manually acknowledging or dismissing system-identified problems, or users controlling annotations by accepting/rejecting recommendations.

#### Adaptive Scaffolding
Dynamically adjusting the level and type of AI assistance provided to users, operating on a spectrum from system-controlled to user-driven approaches, often incorporating hybrid models.
- system-controlled adaptive scaffolding â€“ the AI autonomously modifies support based on pre-defined rules, learned models, or analysis of user behavior and context. Examples include AI suggesting relevant steps based on conversation flow, adjusting guidance based on analyzed user progress, adapting assistance levels to user's skill, tailoring support based on learner understanding, structuring learning based on observed behavior, adapting based on user interaction interpretation, or curating data views based on context.
- user-driven adaptive scaffolding â€“ users explicitly control the adaptation of support mechanisms. They might manually adjust settings, select different assistance levels, request specific types of support, trigger or dismiss system hints, specify proficiency levels, or set
goals to modify assistance intensity.
- hybrid adaptive scaffolding â€“ combines system autonomy with user control, where the system might make adjustments but allow user overrides or modifications. Adaptation factors include user proficiency, task phase or context, explicit user feedback, and observed task progress.

#### Chain-of-Thought
Explicitly display the AIâ€™s step-by-step reasoning process used to reach a solution or suggestion. Making the systemâ€™s logic transparent helps users understand and evaluate the output.

### Feedback

#### Confidence visualization
Communicate AI reliability or influence user confidence regarding AI outputs and actions.

- confidence/uncertainty visualization â€“ interface visually represent the AIâ€™s confidence level (e.g., via scores, intervals or alternatives) for
its outputs or actions. This helps users gauge reliability and make informed decisions about interpreting or acting upon AI contributions.
- user feedback and trust management â€“ systems incorporate feedback channels to measure how design choices influence usersâ€™ perception of system reliability, bias, and overall trust
- confidence-based ranking and prioritization â€“ algorithms order or rank AI suggestions based on calculated confidence metrics. This guides user attention towards reliable items first, as seen in pattern prioritization based on model confidence

#### Explanatory feedback
Transparency strategies to help users understand AI operations and reasoning, facilitating informed decisions.
- model explanation and reasoning disclosure â€“ systems provide explicit insights into AI decision-making. This involves interpreting ambiguous outputs, offering views into model operations, using visualizations like feature importance plots, explaining design outcomes, displaying flags based on rules.
- visual highlighting and differentiation â€“ interfaces use visual cues to distinguish AI contributions or guide user attention. Examples include color-coding AI-written text, highlighting key points in peer reviews, emphasizing generative variability.

#### Iterative feedback loop
Continuous refinement of co-created outputs through dynamic exchanges based on user input or environmental changes.
- user-directed feedback â€“ users explicitly provide feedback to guide AI performance and improvements. Examples include using AI outputs to inform subsequent prompts, offering direct interaction feedback or ratings, providing textual feedback on generated content, iteratively testing outputs with parameter adjustments, manually indicating alignment with assessments.
- system-initiated feedback â€“ systems proactively provide these feedback to enhance the interaction process or guide users. AI might respond to usersâ€™ additional queries, provide real-time suggestions with system adaptations, offer immediate visual feedback, monitor user actions and adjust responses, serve as a feedback tool during ideation, track goals for immediate feedback, propose updated content based on input, or establish user-system response cycles for refinement
- bidirectional interaction feedback â€“ the feedback flows dynamically in both directions between users and the AI

## Relationships to different levels of design system

System agency progressively expands as we move up through scales of the design system:

### [Foundation](../?path=/docs/foundations-overview--docs)

At the  level, basic elements provide limited to no agency.
They're tightly constrained, predictable, and dependableâ€”ensuring everything stays
visually and contextually coherent.

### [Primitives](../?path=/docs/primitives-overview--docs) and [components](../?path=/docs/components-overview--docs)

Assistanceâ€”like setting a default valueâ€”is probably the most typical example of agency on this level.
Primitives can also be dynamically arranged, selected, or surfaced based on context.
A [toolbar](../?path=/docs/components-toolbar--docs) might offer precisely relevant actions in response to user intent,
actively supporting the user's immediate goals without straying too far from familiar interactions.

### [Compositions](../?path=/docs/compositions-overview--docs)
[Messages](../?path=/docs/compositions-messaging--docs) or product [cards](../?path=/docs/compositions-card--docs)â€”can carry greater agency.
They intelligently package sets of components into meaningful units. With increased agency, compositions can adapt themselves dynamically,
appearing precisely where and when users need them, leveraging context.

### [Patterns](../?path=/docs/patterns-overview--docs)
Patterns can adapt and reshape themselves strategically to construct the flow of
an interface and to actively guide the user. [Explanations](../?path=/docs/patterns-explanation--docs), [suggestions](../?path=/docs/patterns-suggestion--docs), or [dynamically
arranged sections](../?path=/docs/patterns-focus-and-context--docs) respond proactively, anticipating user needs and creating experiences that feel
coherent, intentional, and responsive.

## Balancing system and user agency

- **Mental model shift**: Designing agentive software requires viewing software as an active participant rather than passive tool.
- **Delegation vs control**: Strike a balance between leveraging system capabilities and maintaining user oversight. Users should retain the ability to review, modify, and make final decisions.
- **Preserving expertise**: Protect user domain knowledge by ensuring meaningful engagement with tasks. Complete delegation can lead to skill atrophy and loss of critical judgement.
- **Transparency**: System decisions should be explainable and understandable to maintain user trust and enable informed intervention.
- **Graceful degradation**: When system agency fails, users should be able to seamlessly take control without losing context or progress.

## Related

- AX: agent experience
- Relies on [adaptation](../?path=/docs/foundations-adaptation--docs)
- [Bot patterns](../?path=/docs/patterns-bot--docs) demonstrate different agency levels through interaction modes

## Resources & references

- Zhang, Wang, Yi (2025) [Exploring Collaboration Patterns and Strategies in Human-AI Co-creation through the Lens of Agency](https://arxiv.org/abs/2507.06000)
- Nardi, B., Miller, J. R., & Wright, D. J. (1998). [Collaborative, programmable intelligent agents](https://dl.acm.org/doi/10.1145/272287.272331). Communications of the ACM, 41(3), 96â€“104.
- [NN/g / Noncommand user interfaces](https://www.nngroup.com/articles/noncommand/), Jakob Nielsen, 1993
